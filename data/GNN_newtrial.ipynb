{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df5d63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datacleaner import *\n",
    "from abc import ABC\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import dgl\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d92dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of negative values in target_HF column is: 12479\n",
      "The number of positive values in target_HF column is: 7917\n"
     ]
    }
   ],
   "source": [
    "def get_cleaned_df():\n",
    "    df = get_dataframe()\n",
    "    \n",
    "    #Count the number of diagnosis risk factors\n",
    "    icd9_cols = df.filter(regex='^icd9').columns\n",
    "\n",
    "    # Find all columns with age of diagnoses\n",
    "    age_cols = df.filter(regex='admit_age').columns\n",
    "    age_cols = age_cols[:-1] #don't get the HF age cause we don't want to drop that one\n",
    "\n",
    "    # Find all columns with icu stay of diagnoses\n",
    "    icu_cols = df.filter(regex='icu_stay').columns\n",
    "\n",
    "    # Find columns associated with echo data\n",
    "    echo_cols = ['height', 'weight', 'bpsys', 'bpdias', 'hr', 'EF']\n",
    "    \n",
    "    #Create a new dataframe that has the groupings\n",
    "    grouped_df = pd.DataFrame()\n",
    "\n",
    "    # group the diagnoses\n",
    "    for i in range(len(icd9_cols)):\n",
    "        # stack the three columns into a single list column\n",
    "        df['new_'+icd9_cols[i]] = df.apply(lambda x: [x[icd9_cols[i]], x[age_cols[i]], x[icu_cols[i]]], axis=1)\n",
    "        grouped_df['new_'+icd9_cols[i]] = (df['new_'+icd9_cols[i]])\n",
    "        df.drop(('new_'+icd9_cols[i]), axis=1)\n",
    "        \n",
    "    #group all echo variables with age and echo_icu_stay\n",
    "    for i in range(len(echo_cols)):\n",
    "        # stack the three columns into a single list column\n",
    "        df['new_'+echo_cols[i]] = df.apply(lambda x: [x[echo_cols[i]], x['age'], x['echo_icu_stay']], axis=1)\n",
    "        grouped_df['echo_'+echo_cols[i]] = (df['new_'+echo_cols[i]])\n",
    "        df.drop('new_'+echo_cols[i], axis=1)\n",
    "        \n",
    "    #add gender - standalone\n",
    "    # replace 'M' and 'F' with 1 and 2, respectively\n",
    "    df['gender'] = df['gender'].replace({'M': 1, 'F': 2})\n",
    "    grouped_df['gender'] = df.apply(lambda x: [x['gender'], 0, 0], axis=1)\n",
    "\n",
    "    #add target HF\n",
    "    grouped_df['HF'] = df['target_HF']\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "\n",
    "grouped_df = get_cleaned_df()\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "# split data into training and test sets\n",
    "train_data, test_data = train_test_split(grouped_df, test_size=test_size)\n",
    "train_data, val_data = train_test_split(train_data, test_size=val_size/(1-test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46a7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddf120a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset, ABC):\n",
    "    def __init__(self, data_df):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Extract the features and labels\n",
    "        features_tensor = torch.tensor([], dtype=torch.float)\n",
    "        labels_tensor = torch.tensor([], dtype=torch.long)\n",
    "        \n",
    "        # Extract the labels\n",
    "        self.labels = torch.tensor(data_df['HF'].tolist(), dtype=torch.long)\n",
    "        \n",
    "        # Extract the image data\n",
    "        self.data = torch.FloatTensor(data_df.loc[:, data_df.columns != 'HF'].values.tolist())\n",
    "        #print(self.data.shape)\n",
    "        \n",
    "        # Find the proportion of each digit in the set\n",
    "        self.class_weights = 1 / np.unique(self.labels, return_counts=True)[1]\n",
    "        #print(np.unique(self.labels, return_counts=True)[1])\n",
    "        self.class_weights = self.class_weights[self.labels]\n",
    "        #print(self.class_weights)\n",
    "        \n",
    "        self.nx_graph = nx.complete_graph(46) \n",
    "        \n",
    "        self.num_samples = data_df.shape[0]    \n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        # Retrieve the sample\n",
    "        sample_features = self.data[idx].view(46, -1).unsqueeze(0).unsqueeze(0) #TODO\n",
    "\n",
    "        # Retrieve the label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Create the PyG data from the graph structure\n",
    "        #g = from_networkx(self.nx_graph)\n",
    "        #g = dgl.from_networkx(nx_g)\n",
    "        # Create DGL graph from networkx\n",
    "        g = from_networkx(self.nx_graph)\n",
    "\n",
    "        # Add data and label to the PyG data\n",
    "        g.y = label\n",
    "        g.features = sample_features\n",
    "        g.nx_graph = nx.complete_graph(46) \n",
    "\n",
    "        return g\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c8b3c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/_sfmqqbj0sq0f3v44_gt4f7h0000gn/T/ipykernel_26019/2760376088.py:11: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  self.labels = torch.tensor(data_df['HF'].tolist(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MIMICDataset(train_data)\n",
    "val_dataset = MIMICDataset(val_data)\n",
    "test_dataset = MIMICDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c535aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 16316 samples.\n",
      "The training set has 2040 samples.\n",
      "The test set has 2040 samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training set has {} samples.\".format(len(train_dataset)))\n",
    "print(\"The training set has {} samples.\".format(len(val_dataset)))\n",
    "print(\"The test set has {} samples.\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4bcfe570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing, Sequential, GCNConv, global_add_pool, global_mean_pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from math import floor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1a327c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "batch_size = 32 \n",
    "train_sampler = WeightedRandomSampler(weights=train_dataset.class_weights, num_samples=len(train_dataset), \n",
    "                                      replacement=False)\n",
    "val_sampler = WeightedRandomSampler(weights=val_dataset.class_weights, num_samples=len(val_dataset), \n",
    "                                      replacement=False)                                      \n",
    "test_sampler = WeightedRandomSampler(weights=test_dataset.class_weights, num_samples=len(test_dataset), \n",
    "                                     replacement=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f4445e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, sampler=train_sampler)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, sampler=val_sampler)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bee0ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_feature_dim,\n",
    "                 dropout_p,\n",
    "                 gnn_hidden_dims,\n",
    "                 mlp_hidden_dim,\n",
    "                 num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # GNN layers\n",
    "        for gnn_hidden_dim in gnn_hidden_dims:\n",
    "            self.layers.append(Sequential('x, edge_index', [(GCNConv(in_channels=input_feature_dim,\n",
    "                                                                     out_channels=gnn_hidden_dim), 'x, edge_index -> x'),\n",
    "                                                            nn.BatchNorm1d(gnn_hidden_dim),\n",
    "                                                            nn.Dropout(p=dropout_p),\n",
    "                                                            nn.ReLU(inplace=True)]))\n",
    "            input_feature_dim = gnn_hidden_dim\n",
    "\n",
    "        # Output MLP layers\n",
    "        self.output_mlp = nn.Sequential(nn.Linear(in_features=gnn_hidden_dims[-1],\n",
    "                                                  out_features=mlp_hidden_dim),\n",
    "                                        nn.BatchNorm1d(mlp_hidden_dim),\n",
    "                                        nn.Dropout(p=dropout_p),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(in_features=mlp_hidden_dim,\n",
    "                                                  out_features=num_classes),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        h = g.features.squeeze()\n",
    "        h = torch.reshape(h, [h.shape[0]*h.shape[1], -1])\n",
    "        edge_index = g.edge_index\n",
    "        \n",
    "        # GNN layers\n",
    "        for gnn_layer in self.layers:\n",
    "            h = gnn_layer(h, edge_index)\n",
    "\n",
    "        # Pool node embeddings to create the graph embedding\n",
    "        h = global_mean_pool(h, g.batch)\n",
    "\n",
    "        # Output MLP\n",
    "        h = self.output_mlp(h)\n",
    "        h = h.squeeze()\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "842ef87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_classifiers = GCNClassifier(input_feature_dim=3, #TODO\n",
    "                                dropout_p=0.3,\n",
    "                                gnn_hidden_dims=[64, 16],\n",
    "                                mlp_hidden_dim=16,\n",
    "                                num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6e4d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function and the optimizer\n",
    "optimizer = Adam(list(gnn_classifiers.parameters()))\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e4fea2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 509/509 [01:56<00:00,  4.35it/s]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:13<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.5981, Val Loss: 0.6807, Train Acc: 0.6735, Val Acc: 0.6171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 509/509 [01:56<00:00,  4.36it/s]\n",
      "100%|███████████████████████████████████████████| 63/63 [00:13<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.6062, Val Loss: 0.6489, Train Acc: 0.6656, Val Acc: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▋                | 307/509 [01:09<00:45,  4.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(trainloader)):\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Use GNN layers to propagate messages between embeddings #TODO - define data_batch.x and data_batch.y\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch_geometric/data/dataset.py:239\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 239\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36mMIMICDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create the PyG data from the graph structure\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#g = from_networkx(self.nx_graph)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#g = dgl.from_networkx(nx_g)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create DGL graph from networkx\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnx_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Add data and label to the PyG data\u001b[39;00m\n\u001b[1;32m     42\u001b[0m g\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m label\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/torch_geometric/utils/convert.py:207\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[0;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m    206\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mconvert_node_labels_to_integers(G)\n\u001b[0;32m--> 207\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_directed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mis_directed(G) \u001b[38;5;28;01melse\u001b[39;00m G\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(G, (nx\u001b[38;5;241m.\u001b[39mMultiGraph, nx\u001b[38;5;241m.\u001b[39mMultiDiGraph)):\n\u001b[1;32m    210\u001b[0m     edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39medges(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/networkx/classes/graph.py:1699\u001b[0m, in \u001b[0;36mGraph.to_directed\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1697\u001b[0m G\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph))\n\u001b[1;32m   1698\u001b[0m G\u001b[38;5;241m.\u001b[39madd_nodes_from((n, deepcopy(d)) \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m-> 1699\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnbrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.9/site-packages/networkx/classes/digraph.py:793\u001b[0m, in \u001b[0;36mDiGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    791\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(dd)\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_succ[u][v] \u001b[38;5;241m=\u001b[39m datadict\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pred[v][u] \u001b[38;5;241m=\u001b[39m datadict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "wandb.init(project='ELEC571_Project', entity='vctoriawu')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i, data_batch in enumerate(tqdm(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use GNN layers to propagate messages between embeddings #TODO - define data_batch.x and data_batch.y\n",
    "        data_batch.x = data_batch.features\n",
    "        x = gnn_classifiers(data_batch)\n",
    "        y = data_batch.y\n",
    "        y = y.type(torch.float)\n",
    "        \n",
    "        loss = loss_func(x, y)\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save label and prediction\n",
    "        data_batch.y.detach().cpu().numpy()\n",
    "        prediction = x.detach().cpu().numpy().round()\n",
    "        \n",
    "        train_preds.extend(prediction.tolist())\n",
    "        train_labels.extend(data_batch.y.tolist())\n",
    "\n",
    "    train_time = time.time() - train_time\n",
    "\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    test_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data_batch in enumerate(tqdm(valloader)):\n",
    "\n",
    "            # Use GNN layers to propagate messages between pixel embeddings\n",
    "            data_batch.x = data_batch.features\n",
    "            x = gnn_classifiers(data_batch)\n",
    "            y = data_batch.y\n",
    "            y = y.type(torch.float)\n",
    "            loss = loss_func(x, y)\n",
    "            val_loss += loss.detach().cpu().item() * batch_size\n",
    "\n",
    "            # Save label and prediction\n",
    "            data_batch.y.detach().cpu().numpy()\n",
    "            prediction = x.detach().cpu().numpy().round()\n",
    "            \n",
    "            val_preds.extend(prediction.tolist())\n",
    "            val_labels.extend(data_batch.y.tolist())\n",
    "            \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    \n",
    "\n",
    "    test_time = time.time() - test_time\n",
    "    \n",
    "    # Append losses and accuracies to lists for plotting\n",
    "    train_loss /= len(trainloader)*batch_size\n",
    "    val_loss /= len(valloader)*batch_size\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Print epoch information\n",
    "    wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"train_acc\": train_acc, \"val_acc\": val_acc})\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
